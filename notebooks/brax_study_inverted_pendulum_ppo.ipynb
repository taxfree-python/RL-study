{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viXuZH_yM5gP",
        "outputId": "e4919d7b-630e-4791-e5a8-1b2c95db6f1b"
      },
      "outputs": [],
      "source": [
        "!pip install brax flax optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZsVgy0fM3Ml"
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "from brax.envs import inverted_pendulum\n",
        "from brax.io import html\n",
        "from IPython.display import HTML\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "from typing import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt2z8O3kWIS1",
        "outputId": "4c5f609f-6422-4e83-e678-a8cd34137a3a"
      },
      "outputs": [],
      "source": [
        "print(f\"JAX„ÅÆ„Éá„Éï„Ç©„É´„Éà„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ: {jax.default_backend()}\")\n",
        "print(\"JAX„ÅåË™çË≠ò„Åó„Å¶„ÅÑ„Çã„Éá„Éê„Ç§„Çπ:\")\n",
        "print(jax.devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMg-5fQwM79f"
      },
      "outputs": [],
      "source": [
        "# Áí∞Â¢É„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„ÇπÂåñ„Å®JITÂåñ\n",
        "env = inverted_pendulum.InvertedPendulum(backend=\"positional\")\n",
        "# env = inverted_pendulum.InvertedPendulum()\n",
        "jit_env_reset = jax.jit(env.reset)\n",
        "jit_env_step = jax.jit(env.step)\n",
        "\n",
        "# JAX„ÅÆ‰π±Êï∞„Ç≠„Éº\n",
        "key = jax.random.PRNGKey(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbTe9ytSYu6E"
      },
      "source": [
        "## Actor-Critic (PPO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syE3iZ-ONGIU"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    action_size: int\n",
        "    hidden_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        shared = x\n",
        "        for size in self.hidden_sizes:\n",
        "            shared = nn.Dense(features=size)(shared)\n",
        "            shared = nn.relu(shared)\n",
        "\n",
        "        # Actor\n",
        "        loc = nn.Dense(features=self.action_size)(shared)\n",
        "        loc = nn.tanh(loc)\n",
        "\n",
        "        log_std = self.param(\"log_std\", nn.initializers.zeros, (self.action_size,))\n",
        "        scale = jnp.exp(log_std)\n",
        "\n",
        "        # Critic\n",
        "        value = nn.Dense(features=1)(shared)\n",
        "\n",
        "        return loc, scale, jnp.squeeze(value, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QcNawAhTymt"
      },
      "outputs": [],
      "source": [
        "action_size = env.action_size\n",
        "ac_net = ActorCritic(action_size=action_size, hidden_sizes=[64, 64])\n",
        "key, ac_key = jax.random.split(key)\n",
        "dummy_obs = jnp.zeros((1, env.observation_size))\n",
        "params = ac_net.init(ac_key, dummy_obs)[\"params\"]\n",
        "\n",
        "learning_rate = 3e-4\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(1.0), optax.adam(learning_rate=learning_rate)\n",
        ")\n",
        "opt_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TnghpmGNUKp"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(params, opt_state, key):\n",
        "    def rollout_step(carry, _):\n",
        "        state, key = carry\n",
        "        key, policy_key = jax.random.split(key)\n",
        "        loc, scale, value = ac_net.apply({\"params\": params}, state.obs)\n",
        "        dist = tfd.Normal(loc=loc, scale=scale)\n",
        "        action = dist.sample(seed=policy_key)\n",
        "        log_prob = dist.log_prob(action).sum()\n",
        "        next_state = jit_env_step(state, action)\n",
        "        transition = (\n",
        "            state.obs,\n",
        "            action,\n",
        "            log_prob,\n",
        "            value,\n",
        "            next_state.reward,\n",
        "            1.0 - next_state.done,\n",
        "        )\n",
        "\n",
        "        return (next_state, key), transition\n",
        "\n",
        "    def calculate_gae(transitions, last_val):\n",
        "        gamma, lambda_ = 0.99, 0.95\n",
        "\n",
        "        def scan_fn(gae_and_next_val, transition):\n",
        "            gae, next_val = gae_and_next_val\n",
        "            _, _, _, value, reward, done = transition\n",
        "            delta = reward + gamma * next_val * done - value\n",
        "            gae = delta + gamma * lambda_ * gae * done\n",
        "            return (gae, value), gae\n",
        "\n",
        "        _, advantages = jax.lax.scan(\n",
        "            scan_fn, (0.0, last_val), transitions, reverse=True\n",
        "        )\n",
        "        returns = advantages + transitions[3]\n",
        "\n",
        "        return advantages, returns\n",
        "\n",
        "    def loss_fn(params, obs, action, log_prob_old, advantage, return_val):\n",
        "        loc, scale, value_pred = ac_net.apply({\"params\": params}, obs)\n",
        "        dist = tfd.Normal(loc=loc, scale=scale)\n",
        "        log_prob_new = dist.log_prob(action).sum()\n",
        "        ratio = jnp.exp(log_prob_new - log_prob_old)\n",
        "        policy_loss = -jnp.minimum(\n",
        "            ratio * advantage, jnp.clip(ratio, 1.0 - 0.2, 1.0 + 0.2) * advantage\n",
        "        )\n",
        "        value_loss = optax.l2_loss(value_pred, return_val)\n",
        "        return policy_loss.mean() + 0.5 * value_loss.mean()\n",
        "\n",
        "    # Rollout\n",
        "    key, reset_key = jax.random.split(key)\n",
        "    initial_state = jit_env_reset(reset_key)\n",
        "    (final_state, _), transitions = jax.lax.scan(\n",
        "        rollout_step, (initial_state, key), None, length=200\n",
        "    )\n",
        "\n",
        "    # GAE Calculation\n",
        "    _, _, last_val = ac_net.apply({\"params\": params}, final_state.obs)\n",
        "\n",
        "    advantages, returns = calculate_gae(transitions, last_val)\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Loss Calculation and Update\n",
        "    obs_batch, action_batch, log_prob_batch, _, _, _ = transitions\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grads = grad_fn(\n",
        "        params, obs_batch, action_batch, log_prob_batch, advantages, returns\n",
        "    )\n",
        "    updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return new_params, new_opt_state, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG4veMeEXHRP",
        "outputId": "4703b899-09a7-4b52-d3e4-389c213534d4"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Starting training run...\")\n",
        "policy_params = params\n",
        "total_epochs = 500\n",
        "for epoch in range(1, total_epochs + 1):\n",
        "    key, train_key = jax.random.split(key)\n",
        "    policy_params, opt_state, loss = train_step(policy_params, opt_state, train_key)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch: {epoch}/{total_epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "lYJ7L7nnXKCI",
        "outputId": "c288b52b-ed40-4612-818b-3e96d39f3fa1"
      },
      "outputs": [],
      "source": [
        "# --- 6. Evaluation ---\n",
        "print(\"‚úÖ Evaluating trained model...\")\n",
        "\n",
        "# Ensure the jit_policy function is defined\n",
        "jit_policy = jax.jit(\n",
        "    lambda params, state: ac_net.apply({\"params\": params}, state.obs)[0]\n",
        ")\n",
        "\n",
        "key, eval_key = jax.random.split(key)\n",
        "eval_state = jit_env_reset(eval_key)\n",
        "rollout = []\n",
        "\n",
        "for _ in range(1000):  # Evaluate for 1000 steps\n",
        "    rollout.append(eval_state)\n",
        "    action = jit_policy(policy_params, eval_state)\n",
        "    eval_state = jit_env_step(eval_state, action)\n",
        "    if eval_state.done:  # Stop if the episode ends\n",
        "        break\n",
        "\n",
        "# Correctly access the physics state via `pipeline_state`\n",
        "physics_states = [s.pipeline_state for s in rollout]\n",
        "display(HTML(html.render(env.sys, physics_states)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
